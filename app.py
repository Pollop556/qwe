import torch
import pandas as pd
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    TrainingArguments, 
    Trainer, 
    DataCollatorForLanguageModeling
)

def train():
    model_name = "Vikhrmodels/Vikhr-Llama-3.2-1B-instruct"
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right" # –í–∞–∂–Ω–æ –¥–ª—è Llama

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",
        torch_dtype=torch.float16 # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è T4 GPU –≤ Colab
    )

    # 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–ò–¥–µ–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç)
    df = pd.read_csv('shuffled_dataset.csv')

    def format_chat(example):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç Llama-3 –¥–ª—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
        messages = [
            {"role": "system", "content": "–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç-–∫—É—Ä–∞—Ç–æ—Ä. –ü–∏—à–µ—à—å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ –ø–æ –∑–∞–¥–∞–Ω–Ω—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º. –ò—Å–ø–æ–ª—å–∑—É–π –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ-–¥–µ–ª–æ–≤–æ–π —Å—Ç–∏–ª—å."},
            {"role": "user", "content": f"–°—Ñ–æ—Ä–º–∏—Ä—É–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫—É: {example['input']}"},
            {"role": "assistant", "content": example['target']}
        ]
        return {"text": tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)}

    dataset = Dataset.from_pandas(df).map(format_chat)

    def tokenize_func(examples):
        return tokenizer(examples["text"], truncation=True, max_length=384, padding="max_length")

    tokenized_dataset = dataset.map(tokenize_func, batched=True, remove_columns=dataset.column_names)

    # 3. –ò–¥–µ–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è –¥–ª—è 1B –º–æ–¥–µ–ª–∏
    training_args = TrainingArguments(
        output_dir="./vikhr_results",
        num_train_epochs=3,              # 3 —ç–ø–æ—Ö–∏ ‚Äî "–∑–æ–ª–æ—Ç–æ–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç"
        per_device_train_batch_size=2,   # –ß—Ç–æ–±—ã –Ω–µ –≤—ã–ª–µ—Ç–µ—Ç—å –ø–æ –ø–∞–º—è—Ç–∏
        gradient_accumulation_steps=8,   # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –±–∞—Ç—á = 16 (2*8)
        learning_rate=2e-5,              # –ú—è–≥–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ
        lr_scheduler_type="cosine",      # –ü–ª–∞–≤–Ω–æ–µ –∑–∞—Ç—É—Ö–∞–Ω–∏–µ
        warmup_ratio=0.1,                # 10% –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ —Ä–∞–∑–æ–≥—Ä–µ–≤
        weight_decay=0.05,               # –ü—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        logging_steps=5,
        save_strategy="no",
        fp16=True,                       # –£—Å–∫–æ—Ä–µ–Ω–∏–µ –Ω–∞ GPU
        gradient_checkpointing=True,     # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏—è VRAM
        report_to="none"
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
    )

    print("üöÄ –ó–∞–ø—É—Å–∫ –∏–¥–µ–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è Vikhr-1B...")
    trainer.train()
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    model.save_pretrained("./final_vikhr_model")
    tokenizer.save_pretrained("./final_vikhr_model")
    print("‚úÖ –ì–æ—Ç–æ–≤–æ! –ú–æ–¥–µ–ª—å –≤ –ø–∞–ø–∫–µ ./final_vikhr_model")

if __name__ == "__main__":
    train()
